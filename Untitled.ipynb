{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import queue\n",
    "import heapq\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, env_shape, buffer_size, random_seed=123):\n",
    "        \"\"\"\n",
    "        The right side of the deque contains the most recent experiences\n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = []\n",
    "        heapq.heapify(self.buffer)\n",
    "        random.seed(random_seed)\n",
    "        self.env_shape = env_shape\n",
    "        self.c = 0\n",
    "        self.alpha = 0.5\n",
    "\n",
    "    def collect(self, s, a, r, s1, t):\n",
    "\n",
    "        # collecting trajectory\n",
    "        s = np.reshape(s, (-1, self.env_shape[0]))\n",
    "        a = np.reshape(a, (-1, self.env_shape[1]))\n",
    "        s1 = np.reshape(s1, (-1, self.env_shape[0]))\n",
    "\n",
    "        entry = (s, a, r, s1, t)\n",
    "        priority = 1\n",
    "        self.c += 1\n",
    "        # in theory break heap but we rebuild when update\n",
    "        self.buffer.append((priority, self.c, entry))\n",
    "        print(len(self.buffer))\n",
    "\n",
    "\n",
    "    # def add(self, exp):\n",
    "    #\n",
    "    #     if self.count < self.buffer_size:\n",
    "    #         self.buffer.append(exp)\n",
    "    #         self.count += 1\n",
    "    #     else:\n",
    "    #         self.buffer.popleft()\n",
    "    #         self.buffer.append(exp)\n",
    "\n",
    "    def get_size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def update(self, error, sample):\n",
    "\n",
    "        values = heapq.nlargest((self.buffer_size - 100), self.buffer)\n",
    "\n",
    "        priority = list(map(lambda x: x[0], values))\n",
    "        entry = list(map(lambda x: x[2], values))\n",
    "\n",
    "        priority.extend(error)\n",
    "        priority = np.array(priority)\n",
    "        priority = priority/sum(priority)\n",
    "\n",
    "        entry.extend(sample)\n",
    "        \n",
    "        count = [i for i in range(len(entry))]\n",
    "        if self.c < len(count):\n",
    "            self.c = len(count) + 1\n",
    "        else:\n",
    "            self.c += 1\n",
    "\n",
    "        self.buffer = list(zip(priority, count, entry))\n",
    "        print(self.buffer)\n",
    "\n",
    "        heapq.heapify(self.buffer)\n",
    "\n",
    "\n",
    "    def get_sample(self, batch_size):\n",
    "\n",
    "        priority = list(map(lambda x: x[0], self.buffer))\n",
    "        priority = np.array(priority)\n",
    "        priority = priority / sum(priority)\n",
    "        entry = np.array(list(map(lambda x: x[2], self.buffer)))\n",
    "\n",
    "        if len(entry) < batch_size:\n",
    "            ix = np.random.choice(a=len(entry), size=len(entry), replace=True, p=priority)\n",
    "        else:\n",
    "            ix = np.random.choice(a=len(entry), size=batch_size, replace=True, p=priority)\n",
    "\n",
    "        batch = entry[ix,:]\n",
    "        print(batch)\n",
    "        print(batch[:,0])\n",
    "        print(batch[:,1])\n",
    "        print(batch[:,2])\n",
    "        print(batch[:,3])\n",
    "        print(batch[:,4])\n",
    "        s1_batch = np.vstack(batch[:,0])#np.vstack(list(map(lambda x: x[0], batch)))\n",
    "        a_batch = np.vstack(batch[:,1])#np.vstack(np.array(list(map(lambda x: x[1], batch))))\n",
    "        r_batch = np.vstack(batch[:,2]) #np.vstack(np.array(list(map(lambda x: x[2], batch))))\n",
    "        s2_batch = np.vstack(batch[:,3])#np.vstack(np.array(list(map(lambda x: x[3], batch))))\n",
    "        t_batch = np.vstack(batch[:,4])#np.vstack(np.array(list(map(lambda x: x[4], batch))))\n",
    "\n",
    "        return s1_batch, a_batch, r_batch, s2_batch, t_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replay = ReplayBuffer((1,1),1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "replay.collect(1,2,3,4,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.125, 0, (array([[1]]), array([[2]]), 3, array([[4]]), 7)),\n",
       " (0.125, 1, (array([[1]]), array([[2]]), 3, array([[4]]), 7)),\n",
       " (0.125, 2, (array([[1]]), array([[2]]), 3, array([[4]]), 7)),\n",
       " (0.125, 3, (array([[1]]), array([[2]]), 3, array([[4]]), 7)),\n",
       " (0.25, 4, (1, 1, 1, 1, 1)),\n",
       " (0.25, 5, (2, 2, 2, 2, 2))]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.125, 0, (array([[1]]), array([[2]]), 3, array([[4]]), 7)), (0.125, 1, (array([[1]]), array([[2]]), 3, array([[4]]), 7)), (0.125, 2, (array([[1]]), array([[2]]), 3, array([[4]]), 7)), (0.125, 3, (array([[1]]), array([[2]]), 3, array([[4]]), 7)), (0.25, 4, (1, 1, 1, 1, 1)), (0.25, 5, (2, 2, 2, 2, 2))]\n"
     ]
    }
   ],
   "source": [
    "replay.update([2,2], [(1,1,1,1,1), (2,2,2,2,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]\n",
      " [2 2 2 2 2]]\n",
      "[1 2]\n",
      "[1 2]\n",
      "[1 2]\n",
      "[1 2]\n",
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "x = replay.get_sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2]])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.125, 0, (array([[1]]), array([[2]]), 3, array([[4]]), 7)),\n",
       " (0.125, 1, (array([[1]]), array([[2]]), 3, array([[4]]), 7)),\n",
       " (0.125, 2, (array([[1]]), array([[2]]), 3, array([[4]]), 7)),\n",
       " (0.125, 3, (array([[1]]), array([[2]]), 3, array([[4]]), 7)),\n",
       " (0.25, 4, (1, 1, 1, 1, 1)),\n",
       " (0.25, 5, (2, 2, 2, 2, 2))]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-23 21:38:05,494] Making new env: Walker2d-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Worker target ready to go ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-23 21:38:06,690] Worker target ready to go ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Worker local ready to go ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-23 21:38:07,591] Worker local ready to go ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c5e65fac1228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-c5e65fac1228>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthink\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGAMMA\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msummarize\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/georgos/dyn_walk/agent.py\u001b[0m in \u001b[0;36mthink\u001b[0;34m(self, gamma, batch_size, summarize)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0msample_transition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sample\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0ms1_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0ma_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mr_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0ms2_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_transition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/georgos/dyn_walk/replay_buffer.py\u001b[0m in \u001b[0;36mget_sample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpriority\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice (numpy/random/mtrand/mtrand.c:17109)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from agent import Agent\n",
    "from ou_noise import OUNoise\n",
    "# from daddy import Daddy, augment_state\n",
    "import os\n",
    "import gym\n",
    "\n",
    "# from osim.env import RunEnv\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity( tf.logging.INFO )\n",
    "\n",
    "# 'Walker2d-v1'\n",
    "# 'Pendulum-v0'\n",
    "\n",
    "ENV_NAME = 'Walker2d-v1'\n",
    "\n",
    "MEMORY_SIZE = 1e6\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "NUM_EP = 5000\n",
    "SAVE_EVERY = 100\n",
    "H_SIZE = [ 128 , 128 ]\n",
    "PRE_TRAIN = 100\n",
    "IS_STOCHASTIC = True\n",
    "\n",
    "def main():\n",
    "    now = datetime.utcnow().strftime( \"%b-%d_%H_%M\" )  # create unique dir\n",
    "\n",
    "    full_path = os.path.join( os.getcwd() , 'logs' , now )\n",
    "\n",
    "    env = gym.make( ENV_NAME )\n",
    "\n",
    "    # env = RunEnv( visualize=False )\n",
    "    # 5 is the number of velocities for head and other parts\n",
    "    env_dims = (env.observation_space.shape[ 0 ], env.action_space.shape[ 0 ] , (env.action_space.low, env.action_space.high))\n",
    "    ou = OUNoise( action_dimension=env_dims[ 1 ] )\n",
    "\n",
    "    # tf.reset_default_graph ()\n",
    "\n",
    "    target = Agent( name='target' , env_dim=env_dims , h_size=H_SIZE , stochastic=IS_STOCHASTIC )\n",
    "\n",
    "    global_step = tf.Variable( 0 , trainable=False , name='global_step' )\n",
    "    writer = tf.summary.FileWriter( full_path )\n",
    "    saver = tf.train.Saver( tf.get_collection( tf.GraphKeys.GLOBAL_VARIABLES , scope='target' ) , max_to_keep=2 )\n",
    "    ckpt = tf.train.latest_checkpoint( full_path )\n",
    "\n",
    "    agent = Agent( name='local' , env_dim=env_dims , target=target , writer=writer , h_size=H_SIZE , stochastic=IS_STOCHASTIC )\n",
    "\n",
    "    # daddy = Daddy( target=agent , env_dim=env_dims )\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        if ckpt:\n",
    "            tf.logging.info('Restore model {}'.format(ckpt))\n",
    "            saver.restore(sess=sess,  save_path=ckpt)\n",
    "\n",
    "        sess.run( tf.global_variables_initializer() )\n",
    "\n",
    "        summarize = False\n",
    "        # load pre trained model\n",
    "        #\n",
    "        # for _ in range(PRE_TRAIN):\n",
    "        #     l, tot_rw, timesteps = daddy.sample(env= env, w = daddy.w)\n",
    "        #\n",
    "        #     if _ % 5 == 0:\n",
    "        #         ep_summary = tf.Summary()\n",
    "        #\n",
    "        #         ep_summary.value.add( simple_value=tot_rw , tag='daddy/total_rw' )\n",
    "        #         ep_summary.value.add( simple_value=timesteps , tag='daddy/timesteps' )\n",
    "        #         ep_summary.value.add( simple_value=l , tag='daddy/loss' )\n",
    "        #\n",
    "        #         agent.writer.add_summary( ep_summary , _ )\n",
    "        #         agent.writer.flush()\n",
    "        #\n",
    "        #         tf.logging.info(\n",
    "        #             'Master ep  {}, latest ep reward {}, of steps {}'.format( _ , tot_rw , timesteps ) )\n",
    "        #\n",
    "        # tf.logging.info('Pre-train ended, starting training now...')\n",
    "        # # save memory\n",
    "        # daddy.save_memory('memory.pkl')\n",
    "        # # transfer memory\n",
    "        # agent.memory.buffer = copy.deepcopy(daddy.memory.buffer)\n",
    "        #\n",
    "        # saver.save( sess , os.path.join( full_path , 'model.ckpt' ) , global_step=PRE_TRAIN )\n",
    "\n",
    "        for ep in range( NUM_EP ):\n",
    "\n",
    "            agent.sync()\n",
    "            state = env.reset()\n",
    "            ou.reset()\n",
    "\n",
    "            terminal = False\n",
    "\n",
    "            timesteps , tot_rw = 0 , 0\n",
    "\n",
    "\n",
    "            # activate if osim_rl\n",
    "            # state = augment_state( state , state )\n",
    "\n",
    "            while not terminal:\n",
    "                # if stochastic remove exploration noise\n",
    "                action = agent.get_action( state )  # + ou.noise()\n",
    "\n",
    "                # if determinstic clip here\n",
    "                # action = np.clip(action, 0, 1)\n",
    "\n",
    "                next_state , reward , terminal , _ = env.step( action.flatten() )\n",
    "\n",
    "                # Activate if osim-rl\n",
    "                # rw = surr_rw( state , action ) + reward\n",
    "                # next_state = augment_state( state , next_state )\n",
    "\n",
    "                agent.memory.collect( state , action , reward , next_state , terminal )\n",
    "                agent.think( batch_size=BATCH_SIZE , gamma=GAMMA , summarize=summarize )\n",
    "\n",
    "                state = next_state\n",
    "                summarize = False\n",
    "\n",
    "                timesteps += 1\n",
    "                tot_rw += reward\n",
    "\n",
    "            if ep % 5 == 0:\n",
    "                summarize = True\n",
    "                ep_summary = tf.Summary()\n",
    "\n",
    "                ep_summary.value.add( simple_value=tot_rw , tag='eval/total_rw' )\n",
    "                ep_summary.value.add( simple_value=timesteps , tag='eval/ep_length' )\n",
    "\n",
    "                agent.writer.add_summary( ep_summary , ep )\n",
    "                agent.writer.flush()\n",
    "\n",
    "                tf.logging.info(\n",
    "                    'Master ep  {}, latest ep reward {}, of steps {}'.format( ep , tot_rw , timesteps ) )\n",
    "\n",
    "            if ep % SAVE_EVERY == 0:\n",
    "                gs = tf.train.global_step( sess , global_step )\n",
    "                saver.save( sess , os.path.join( full_path , 'model.ckpt' ) , global_step=gs )\n",
    "                tf.logging.info( 'Model saved at ep {}'.format( gs ) )\n",
    "\n",
    "\n",
    "def augment_state(s , s1):\n",
    "\n",
    "    s = np.reshape( np.array( s ) , (1 , -1) )\n",
    "    s1 = np.reshape( np.array( s1 ) , (1 , -1) )\n",
    "\n",
    "    idxs = [ 22 , 24 , 26 , 28 , 30 ]\n",
    "\n",
    "    vel = (s1[ : , idxs ] - s[ : , idxs ]) / (0.01)\n",
    "    # keep information of the environment like difficulty\n",
    "    return np.reshape( np.append( s1[:,:38] , np.append(vel, s1[:,38:])) , (1 , -1) )\n",
    "\n",
    "\n",
    "def surr_rw(state , action):\n",
    "    # state = np.array(state)\n",
    "    delta_h = state[ :,27 ] - state[ :,35 ]\n",
    "    rw = 10 * state[ :,20 ] - abs( delta_h - 1.2 ) - 0.1 * np.linalg.norm( action ) - 10 * (state[ :,27 ] < 0.8)\n",
    "    return np.asscalar( rw )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
